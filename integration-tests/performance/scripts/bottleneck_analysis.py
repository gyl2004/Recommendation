#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç³»ç»Ÿç“¶é¢ˆåˆ†æžè„šæœ¬
åˆ†æžæ€§èƒ½æµ‹è¯•ç»“æžœï¼Œè¯†åˆ«ç³»ç»Ÿç“¶é¢ˆå¹¶æä¾›ä¼˜åŒ–å»ºè®®
"""

import sys
import json
import psutil
import time
import subprocess
import mysql.connector
import redis
from datetime import datetime
from collections import defaultdict

class BottleneckAnalyzer:
    def __init__(self):
        self.analysis_results = {}
        self.recommendations = []
    
    def analyze_system_resources(self):
        """åˆ†æžç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        print("ðŸ” åˆ†æžç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ...")
        
        # CPUåˆ†æž
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_count = psutil.cpu_count()
        cpu_freq = psutil.cpu_freq()
        
        # å†…å­˜åˆ†æž
        memory = psutil.virtual_memory()
        swap = psutil.swap_memory()
        
        # ç£ç›˜åˆ†æž
        disk_usage = psutil.disk_usage('/')
        disk_io = psutil.disk_io_counters()
        
        # ç½‘ç»œåˆ†æž
        network_io = psutil.net_io_counters()
        
        system_analysis = {
            'cpu': {
                'usage_percent': cpu_percent,
                'core_count': cpu_count,
                'frequency_mhz': cpu_freq.current if cpu_freq else 0,
                'bottleneck': cpu_percent > 80
            },
            'memory': {
                'total_gb': round(memory.total / (1024**3), 2),
                'used_gb': round(memory.used / (1024**3), 2),
                'usage_percent': memory.percent,
                'available_gb': round(memory.available / (1024**3), 2),
                'bottleneck': memory.percent > 85
            },
            'disk': {
                'total_gb': round(disk_usage.total / (1024**3), 2),
                'used_gb': round(disk_usage.used / (1024**3), 2),
                'usage_percent': (disk_usage.used / disk_usage.total) * 100,
                'read_mb_s': round(disk_io.read_bytes / (1024**2), 2) if disk_io else 0,
                'write_mb_s': round(disk_io.write_bytes / (1024**2), 2) if disk_io else 0,
                'bottleneck': (disk_usage.used / disk_usage.total) * 100 > 90
            },
            'network': {
                'bytes_sent_mb': round(network_io.bytes_sent / (1024**2), 2),
                'bytes_recv_mb': round(network_io.bytes_recv / (1024**2), 2),
                'packets_sent': network_io.packets_sent,
                'packets_recv': network_io.packets_recv,
                'errors': network_io.errin + network_io.errout
            }
        }
        
        self.analysis_results['system_resources'] = system_analysis
        
        # ç”Ÿæˆç³»ç»Ÿèµ„æºä¼˜åŒ–å»ºè®®
        if system_analysis['cpu']['bottleneck']:
            self.recommendations.append({
                'category': 'CPUä¼˜åŒ–',
                'priority': 'high',
                'issue': f"CPUä½¿ç”¨çŽ‡è¿‡é«˜: {cpu_percent}%",
                'suggestions': [
                    'å¢žåŠ CPUæ ¸å¿ƒæ•°æˆ–å‡çº§CPU',
                    'ä¼˜åŒ–åº”ç”¨ç¨‹åºç®—æ³•å¤æ‚åº¦',
                    'å®žçŽ°è´Ÿè½½å‡è¡¡åˆ†æ•£CPUåŽ‹åŠ›',
                    'ä½¿ç”¨å¼‚æ­¥å¤„ç†å‡å°‘CPUé˜»å¡ž'
                ]
            })
        
        if system_analysis['memory']['bottleneck']:
            self.recommendations.append({
                'category': 'å†…å­˜ä¼˜åŒ–',
                'priority': 'high',
                'issue': f"å†…å­˜ä½¿ç”¨çŽ‡è¿‡é«˜: {memory.percent}%",
                'suggestions': [
                    'å¢žåŠ ç³»ç»Ÿå†…å­˜',
                    'ä¼˜åŒ–JVMå †å†…å­˜é…ç½®',
                    'å®žçŽ°æ›´æœ‰æ•ˆçš„ç¼“å­˜ç­–ç•¥',
                    'æ£€æŸ¥å†…å­˜æ³„æ¼é—®é¢˜'
                ]
            })
        
        if system_analysis['disk']['bottleneck']:
            self.recommendations.append({
                'category': 'ç£ç›˜ä¼˜åŒ–',
                'priority': 'medium',
                'issue': f"ç£ç›˜ä½¿ç”¨çŽ‡è¿‡é«˜: {system_analysis['disk']['usage_percent']:.1f}%",
                'suggestions': [
                    'æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶',
                    'å®žçŽ°æ•°æ®å½’æ¡£ç­–ç•¥',
                    'ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨è®¾å¤‡(SSD)',
                    'å®žçŽ°æ•°æ®åˆ†åŒºå­˜å‚¨'
                ]
            })
    
    def analyze_jvm_performance(self):
        """åˆ†æžJVMæ€§èƒ½"""
        print("ðŸ” åˆ†æžJVMæ€§èƒ½...")
        
        try:
            # èŽ·å–Javaè¿›ç¨‹
            java_processes = []
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                if 'java' in proc.info['name'].lower():
                    java_processes.append(proc.info)
            
            jvm_analysis = {}
            for proc in java_processes:
                pid = proc['pid']
                
                # èŽ·å–JVMç»Ÿè®¡ä¿¡æ¯
                try:
                    # ä½¿ç”¨jstatèŽ·å–GCä¿¡æ¯
                    jstat_result = subprocess.run(
                        ['jstat', '-gc', str(pid)],
                        capture_output=True, text=True, timeout=10
                    )
                    
                    if jstat_result.returncode == 0:
                        lines = jstat_result.stdout.strip().split('\n')
                        if len(lines) >= 2:
                            headers = lines[0].split()
                            values = lines[1].split()
                            gc_data = dict(zip(headers, values))
                            
                            # è®¡ç®—å †å†…å­˜ä½¿ç”¨æƒ…å†µ
                            heap_used = float(gc_data.get('EU', 0)) + float(gc_data.get('OU', 0))
                            heap_total = float(gc_data.get('EC', 0)) + float(gc_data.get('OC', 0))
                            heap_usage_percent = (heap_used / heap_total * 100) if heap_total > 0 else 0
                            
                            jvm_analysis[pid] = {
                                'heap_used_mb': round(heap_used / 1024, 2),
                                'heap_total_mb': round(heap_total / 1024, 2),
                                'heap_usage_percent': round(heap_usage_percent, 2),
                                'gc_count': int(gc_data.get('YGC', 0)) + int(gc_data.get('FGC', 0)),
                                'gc_time': float(gc_data.get('YGCT', 0)) + float(gc_data.get('FGCT', 0)),
                                'bottleneck': heap_usage_percent > 85
                            }
                
                except (subprocess.TimeoutExpired, subprocess.CalledProcessError, ValueError):
                    continue
            
            self.analysis_results['jvm_performance'] = jvm_analysis
            
            # ç”ŸæˆJVMä¼˜åŒ–å»ºè®®
            for pid, analysis in jvm_analysis.items():
                if analysis['bottleneck']:
                    self.recommendations.append({
                        'category': 'JVMä¼˜åŒ–',
                        'priority': 'high',
                        'issue': f"JVMå †å†…å­˜ä½¿ç”¨çŽ‡è¿‡é«˜: {analysis['heap_usage_percent']}% (PID: {pid})",
                        'suggestions': [
                            'å¢žåŠ JVMå †å†…å­˜å¤§å° (-Xmxå‚æ•°)',
                            'ä¼˜åŒ–åžƒåœ¾å›žæ”¶å™¨é…ç½®',
                            'æ£€æŸ¥å†…å­˜æ³„æ¼é—®é¢˜',
                            'ä¼˜åŒ–å¯¹è±¡åˆ›å»ºå’Œç¼“å­˜ç­–ç•¥'
                        ]
                    })
        
        except Exception as e:
            print(f"JVMæ€§èƒ½åˆ†æžå¤±è´¥: {e}")
    
    def analyze_database_performance(self):
        """åˆ†æžæ•°æ®åº“æ€§èƒ½"""
        print("ðŸ” åˆ†æžæ•°æ®åº“æ€§èƒ½...")
        
        try:
            # MySQLæ€§èƒ½åˆ†æž
            mysql_config = {
                'host': 'localhost',
                'port': 3306,
                'user': 'root',
                'password': '123456',
                'database': 'recommendation_db'
            }
            
            conn = mysql.connector.connect(**mysql_config)
            cursor = conn.cursor(dictionary=True)
            
            # èŽ·å–æ•°æ®åº“çŠ¶æ€
            cursor.execute("SHOW STATUS LIKE 'Threads_connected'")
            threads_connected = cursor.fetchone()['Value']
            
            cursor.execute("SHOW STATUS LIKE 'Threads_running'")
            threads_running = cursor.fetchone()['Value']
            
            cursor.execute("SHOW STATUS LIKE 'Queries'")
            total_queries = cursor.fetchone()['Value']
            
            cursor.execute("SHOW STATUS LIKE 'Slow_queries'")
            slow_queries = cursor.fetchone()['Value']
            
            # èŽ·å–InnoDBçŠ¶æ€
            cursor.execute("SHOW STATUS LIKE 'Innodb_buffer_pool_read_requests'")
            buffer_read_requests = cursor.fetchone()['Value']
            
            cursor.execute("SHOW STATUS LIKE 'Innodb_buffer_pool_reads'")
            buffer_reads = cursor.fetchone()['Value']
            
            # è®¡ç®—ç¼“å†²æ± å‘½ä¸­çŽ‡
            buffer_hit_rate = (1 - int(buffer_reads) / int(buffer_read_requests)) * 100 if int(buffer_read_requests) > 0 else 0
            
            mysql_analysis = {
                'connections': {
                    'connected': int(threads_connected),
                    'running': int(threads_running),
                    'bottleneck': int(threads_connected) > 100
                },
                'queries': {
                    'total': int(total_queries),
                    'slow': int(slow_queries),
                    'slow_ratio': (int(slow_queries) / int(total_queries)) * 100 if int(total_queries) > 0 else 0,
                    'bottleneck': (int(slow_queries) / int(total_queries)) * 100 > 1 if int(total_queries) > 0 else False
                },
                'buffer_pool': {
                    'hit_rate': round(buffer_hit_rate, 2),
                    'bottleneck': buffer_hit_rate < 95
                }
            }
            
            cursor.close()
            conn.close()
            
            self.analysis_results['mysql_performance'] = mysql_analysis
            
            # ç”ŸæˆMySQLä¼˜åŒ–å»ºè®®
            if mysql_analysis['connections']['bottleneck']:
                self.recommendations.append({
                    'category': 'MySQLè¿žæŽ¥ä¼˜åŒ–',
                    'priority': 'high',
                    'issue': f"MySQLè¿žæŽ¥æ•°è¿‡é«˜: {mysql_analysis['connections']['connected']}",
                    'suggestions': [
                        'ä¼˜åŒ–è¿žæŽ¥æ± é…ç½®',
                        'å¢žåŠ max_connectionså‚æ•°',
                        'æ£€æŸ¥è¿žæŽ¥æ³„æ¼é—®é¢˜',
                        'å®žçŽ°è¿žæŽ¥å¤ç”¨'
                    ]
                })
            
            if mysql_analysis['queries']['bottleneck']:
                self.recommendations.append({
                    'category': 'MySQLæŸ¥è¯¢ä¼˜åŒ–',
                    'priority': 'high',
                    'issue': f"æ…¢æŸ¥è¯¢æ¯”ä¾‹è¿‡é«˜: {mysql_analysis['queries']['slow_ratio']:.2f}%",
                    'suggestions': [
                        'ä¼˜åŒ–æ…¢æŸ¥è¯¢SQL',
                        'æ·»åŠ å¿…è¦çš„ç´¢å¼•',
                        'åˆ†æžæŸ¥è¯¢æ‰§è¡Œè®¡åˆ’',
                        'è€ƒè™‘æŸ¥è¯¢ç¼“å­˜'
                    ]
                })
            
            if mysql_analysis['buffer_pool']['bottleneck']:
                self.recommendations.append({
                    'category': 'MySQLç¼“å†²æ± ä¼˜åŒ–',
                    'priority': 'medium',
                    'issue': f"InnoDBç¼“å†²æ± å‘½ä¸­çŽ‡è¿‡ä½Ž: {mysql_analysis['buffer_pool']['hit_rate']}%",
                    'suggestions': [
                        'å¢žåŠ innodb_buffer_pool_size',
                        'ä¼˜åŒ–æ•°æ®è®¿é—®æ¨¡å¼',
                        'è€ƒè™‘æ•°æ®åˆ†åŒº',
                        'é¢„çƒ­ç¼“å†²æ± '
                    ]
                })
        
        except Exception as e:
            print(f"MySQLæ€§èƒ½åˆ†æžå¤±è´¥: {e}")
    
    def analyze_redis_performance(self):
        """åˆ†æžRedisæ€§èƒ½"""
        print("ðŸ” åˆ†æžRedisæ€§èƒ½...")
        
        try:
            r = redis.Redis(host='localhost', port=6379, decode_responses=True)
            
            # èŽ·å–Redisä¿¡æ¯
            info = r.info()
            
            redis_analysis = {
                'memory': {
                    'used_mb': round(info['used_memory'] / (1024**2), 2),
                    'peak_mb': round(info['used_memory_peak'] / (1024**2), 2),
                    'fragmentation_ratio': info.get('mem_fragmentation_ratio', 1.0),
                    'bottleneck': info.get('mem_fragmentation_ratio', 1.0) > 1.5
                },
                'connections': {
                    'connected_clients': info['connected_clients'],
                    'blocked_clients': info.get('blocked_clients', 0),
                    'bottleneck': info['connected_clients'] > 1000
                },
                'performance': {
                    'keyspace_hits': info.get('keyspace_hits', 0),
                    'keyspace_misses': info.get('keyspace_misses', 0),
                    'hit_rate': 0,
                    'ops_per_sec': info.get('instantaneous_ops_per_sec', 0),
                    'bottleneck': False
                }
            }
            
            # è®¡ç®—å‘½ä¸­çŽ‡
            total_requests = redis_analysis['performance']['keyspace_hits'] + redis_analysis['performance']['keyspace_misses']
            if total_requests > 0:
                hit_rate = (redis_analysis['performance']['keyspace_hits'] / total_requests) * 100
                redis_analysis['performance']['hit_rate'] = round(hit_rate, 2)
                redis_analysis['performance']['bottleneck'] = hit_rate < 90
            
            self.analysis_results['redis_performance'] = redis_analysis
            
            # ç”ŸæˆRedisä¼˜åŒ–å»ºè®®
            if redis_analysis['memory']['bottleneck']:
                self.recommendations.append({
                    'category': 'Rediså†…å­˜ä¼˜åŒ–',
                    'priority': 'medium',
                    'issue': f"Rediså†…å­˜ç¢Žç‰‡çŽ‡è¿‡é«˜: {redis_analysis['memory']['fragmentation_ratio']:.2f}",
                    'suggestions': [
                        'å¯ç”¨å†…å­˜ç¢Žç‰‡æ•´ç†',
                        'ä¼˜åŒ–æ•°æ®ç»“æž„ä½¿ç”¨',
                        'è°ƒæ•´å†…å­˜åˆ†é…ç­–ç•¥',
                        'è€ƒè™‘é‡å¯RedisæœåŠ¡'
                    ]
                })
            
            if redis_analysis['connections']['bottleneck']:
                self.recommendations.append({
                    'category': 'Redisè¿žæŽ¥ä¼˜åŒ–',
                    'priority': 'high',
                    'issue': f"Redisè¿žæŽ¥æ•°è¿‡é«˜: {redis_analysis['connections']['connected_clients']}",
                    'suggestions': [
                        'ä¼˜åŒ–è¿žæŽ¥æ± é…ç½®',
                        'å¢žåŠ maxclientså‚æ•°',
                        'æ£€æŸ¥è¿žæŽ¥æ³„æ¼',
                        'å®žçŽ°è¿žæŽ¥å¤ç”¨'
                    ]
                })
            
            if redis_analysis['performance']['bottleneck']:
                self.recommendations.append({
                    'category': 'Redisç¼“å­˜ä¼˜åŒ–',
                    'priority': 'high',
                    'issue': f"Redisç¼“å­˜å‘½ä¸­çŽ‡è¿‡ä½Ž: {redis_analysis['performance']['hit_rate']}%",
                    'suggestions': [
                        'ä¼˜åŒ–ç¼“å­˜ç­–ç•¥',
                        'è°ƒæ•´ç¼“å­˜è¿‡æœŸæ—¶é—´',
                        'å®žçŽ°ç¼“å­˜é¢„çƒ­',
                        'åˆ†æžç¼“å­˜è®¿é—®æ¨¡å¼'
                    ]
                })
        
        except Exception as e:
            print(f"Redisæ€§èƒ½åˆ†æžå¤±è´¥: {e}")
    
    def analyze_application_performance(self, jtl_file):
        """åˆ†æžåº”ç”¨æ€§èƒ½"""
        print("ðŸ” åˆ†æžåº”ç”¨æ€§èƒ½...")
        
        try:
            # è¯»å–JTLæµ‹è¯•ç»“æžœ
            with open(jtl_file, 'r', encoding='utf-8') as f:
                import csv
                reader = csv.DictReader(f)
                results = list(reader)
            
            if not results:
                print("æœªæ‰¾åˆ°æµ‹è¯•ç»“æžœæ•°æ®")
                return
            
            # åˆ†æžå“åº”æ—¶é—´åˆ†å¸ƒ
            response_times = [int(r['elapsed']) for r in results]
            response_times.sort()
            
            # åˆ†æžé”™è¯¯åˆ†å¸ƒ
            errors = [r for r in results if r['success'] != 'true']
            error_codes = defaultdict(int)
            for error in errors:
                error_codes[error['responseCode']] += 1
            
            # åˆ†æžæŽ¥å£æ€§èƒ½
            endpoint_performance = defaultdict(list)
            for result in results:
                endpoint_performance[result['label']].append(int(result['elapsed']))
            
            app_analysis = {
                'response_time': {
                    'avg': sum(response_times) / len(response_times),
                    'p50': response_times[int(len(response_times) * 0.5)],
                    'p95': response_times[int(len(response_times) * 0.95)],
                    'p99': response_times[int(len(response_times) * 0.99)],
                    'max': max(response_times),
                    'bottleneck': response_times[int(len(response_times) * 0.95)] > 500
                },
                'error_rate': {
                    'total_requests': len(results),
                    'error_requests': len(errors),
                    'error_rate_percent': (len(errors) / len(results)) * 100,
                    'error_codes': dict(error_codes),
                    'bottleneck': (len(errors) / len(results)) * 100 > 1
                },
                'endpoint_analysis': {}
            }
            
            # åˆ†æžå„æŽ¥å£æ€§èƒ½
            for endpoint, times in endpoint_performance.items():
                times.sort()
                app_analysis['endpoint_analysis'][endpoint] = {
                    'avg_response_time': sum(times) / len(times),
                    'p95_response_time': times[int(len(times) * 0.95)],
                    'request_count': len(times),
                    'bottleneck': times[int(len(times) * 0.95)] > 500
                }
            
            self.analysis_results['application_performance'] = app_analysis
            
            # ç”Ÿæˆåº”ç”¨æ€§èƒ½ä¼˜åŒ–å»ºè®®
            if app_analysis['response_time']['bottleneck']:
                self.recommendations.append({
                    'category': 'åº”ç”¨å“åº”æ—¶é—´ä¼˜åŒ–',
                    'priority': 'high',
                    'issue': f"95%å“åº”æ—¶é—´è¿‡é•¿: {app_analysis['response_time']['p95']}ms",
                    'suggestions': [
                        'ä¼˜åŒ–ä¸šåŠ¡é€»è¾‘å¤æ‚åº¦',
                        'å®žçŽ°å¼‚æ­¥å¤„ç†',
                        'ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢',
                        'å¢žåŠ ç¼“å­˜å±‚'
                    ]
                })
            
            if app_analysis['error_rate']['bottleneck']:
                self.recommendations.append({
                    'category': 'åº”ç”¨é”™è¯¯çŽ‡ä¼˜åŒ–',
                    'priority': 'high',
                    'issue': f"é”™è¯¯çŽ‡è¿‡é«˜: {app_analysis['error_rate']['error_rate_percent']:.2f}%",
                    'suggestions': [
                        'åˆ†æžé”™è¯¯æ—¥å¿—',
                        'å®žçŽ°ç†”æ–­æœºåˆ¶',
                        'å¢žåŠ é‡è¯•é€»è¾‘',
                        'ä¼˜åŒ–å¼‚å¸¸å¤„ç†'
                    ]
                })
        
        except Exception as e:
            print(f"åº”ç”¨æ€§èƒ½åˆ†æžå¤±è´¥: {e}")
    
    def generate_optimization_plan(self):
        """ç”Ÿæˆä¼˜åŒ–è®¡åˆ’"""
        print("ðŸ“‹ ç”Ÿæˆç³»ç»Ÿä¼˜åŒ–è®¡åˆ’...")
        
        # æŒ‰ä¼˜å…ˆçº§æŽ’åºå»ºè®®
        high_priority = [r for r in self.recommendations if r['priority'] == 'high']
        medium_priority = [r for r in self.recommendations if r['priority'] == 'medium']
        low_priority = [r for r in self.recommendations if r['priority'] == 'low']
        
        optimization_plan = {
            'immediate_actions': high_priority,
            'short_term_actions': medium_priority,
            'long_term_actions': low_priority,
            'estimated_impact': self._estimate_optimization_impact()
        }
        
        return optimization_plan
    
    def _estimate_optimization_impact(self):
        """ä¼°ç®—ä¼˜åŒ–å½±å“"""
        impact_estimation = {
            'performance_improvement': '20-40%',
            'response_time_reduction': '30-50%',
            'error_rate_reduction': '50-80%',
            'resource_utilization_improvement': '15-30%'
        }
        
        return impact_estimation
    
    def generate_report(self, jtl_file=None):
        """ç”Ÿæˆå®Œæ•´çš„ç“¶é¢ˆåˆ†æžæŠ¥å‘Š"""
        print("=" * 60)
        print("æ™ºèƒ½å†…å®¹æŽ¨èå¹³å°ç³»ç»Ÿç“¶é¢ˆåˆ†æžæŠ¥å‘Š")
        print("=" * 60)
        print(f"åˆ†æžæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # æ‰§è¡Œå„é¡¹åˆ†æž
        self.analyze_system_resources()
        self.analyze_jvm_performance()
        self.analyze_database_performance()
        self.analyze_redis_performance()
        
        if jtl_file:
            self.analyze_application_performance(jtl_file)
        
        # ç”Ÿæˆä¼˜åŒ–è®¡åˆ’
        optimization_plan = self.generate_optimization_plan()
        
        # è¾“å‡ºåˆ†æžç»“æžœ
        print("ðŸ” ç³»ç»Ÿèµ„æºåˆ†æž")
        print("-" * 30)
        system_resources = self.analysis_results.get('system_resources', {})
        if system_resources:
            print(f"CPUä½¿ç”¨çŽ‡: {system_resources['cpu']['usage_percent']}%")
            print(f"å†…å­˜ä½¿ç”¨çŽ‡: {system_resources['memory']['usage_percent']}%")
            print(f"ç£ç›˜ä½¿ç”¨çŽ‡: {system_resources['disk']['usage_percent']:.1f}%")
        print()
        
        print("ðŸ” æ•°æ®åº“æ€§èƒ½åˆ†æž")
        print("-" * 30)
        mysql_perf = self.analysis_results.get('mysql_performance', {})
        if mysql_perf:
            print(f"MySQLè¿žæŽ¥æ•°: {mysql_perf['connections']['connected']}")
            print(f"æ…¢æŸ¥è¯¢æ¯”ä¾‹: {mysql_perf['queries']['slow_ratio']:.2f}%")
            print(f"ç¼“å†²æ± å‘½ä¸­çŽ‡: {mysql_perf['buffer_pool']['hit_rate']}%")
        print()
        
        print("ðŸ” ç¼“å­˜æ€§èƒ½åˆ†æž")
        print("-" * 30)
        redis_perf = self.analysis_results.get('redis_performance', {})
        if redis_perf:
            print(f"Rediså†…å­˜ä½¿ç”¨: {redis_perf['memory']['used_mb']}MB")
            print(f"Redisè¿žæŽ¥æ•°: {redis_perf['connections']['connected_clients']}")
            print(f"ç¼“å­˜å‘½ä¸­çŽ‡: {redis_perf['performance']['hit_rate']}%")
        print()
        
        # è¾“å‡ºä¼˜åŒ–å»ºè®®
        print("ðŸ’¡ ç³»ç»Ÿä¼˜åŒ–å»ºè®®")
        print("-" * 30)
        
        if optimization_plan['immediate_actions']:
            print("ðŸš¨ ç´§æ€¥ä¼˜åŒ– (ç«‹å³æ‰§è¡Œ):")
            for i, action in enumerate(optimization_plan['immediate_actions'], 1):
                print(f"{i}. {action['category']}: {action['issue']}")
                for suggestion in action['suggestions'][:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªå»ºè®®
                    print(f"   - {suggestion}")
                print()
        
        if optimization_plan['short_term_actions']:
            print("âš¡ çŸ­æœŸä¼˜åŒ– (1-2å‘¨å†…):")
            for i, action in enumerate(optimization_plan['short_term_actions'], 1):
                print(f"{i}. {action['category']}: {action['issue']}")
                for suggestion in action['suggestions'][:2]:
                    print(f"   - {suggestion}")
                print()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'analysis_results': self.analysis_results,
            'optimization_plan': optimization_plan,
            'recommendations': self.recommendations
        }
        
        report_file = 'bottleneck_analysis_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ðŸ“„ è¯¦ç»†åˆ†æžæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
        print("=" * 60)

def main():
    if len(sys.argv) > 1:
        jtl_file = sys.argv[1]
    else:
        jtl_file = None
    
    analyzer = BottleneckAnalyzer()
    analyzer.generate_report(jtl_file)

if __name__ == '__main__':
    main()